{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22594f4d-c0af-4a9f-9c4c-1bfa1c7ce40c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "“A simplified ETL process for extracting, transforming, and loading data from AWS S3 to Snowflake, utilizing Amazon RDS, Apache Spark, Databricks, and Delta tables.”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b02d94f5-3e07-46f0-874d-517d0a2ace0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Execute Utilities Notebook\n",
    "\n",
    "- This Notebook(Utilities) contains utility functions that manage logging functionalities and handling metadata related to the tables used in the data pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77965e83-0eaa-41c7-8800-ceb189ca8c35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Projects/ETL/Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3b021e3-ddcd-41d4-b629-d4c86d8a0f6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Execute Secrets Notebook\n",
    "\n",
    "- This notebook (Secrets) contains the required credentials for accessing various services\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1490d392-61de-4d97-a96f-070d3bb2464a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Projects/ETL/Secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d4d22c-60e7-4183-a410-e52c5af12897",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Define the Logger\n",
    "\n",
    "- The logger is configured to capture and store logs in AWS CloudWatch..\n",
    "- Log messages include important runtime information, error details, and execution traces to aid in debugging and system analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879491d6-5a71-4582-aef4-d218d0df77db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logger Creation and Source Metadata Retrieval\n",
    "logger = createLogger(\"Data-PipeLine\",\"general-logger\")  \n",
    "func_logger = createLogger(\"Function-Logger\",\"function-logger\") \n",
    "# get source meta data details \n",
    "table_definitions , table_names , s3_file_keys = get_source_meta_data()   \n",
    "\n",
    "\n",
    "# define logger \n",
    "func = Logger() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0404217-887c-4f27-a648-43efcbc64e99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## import neccessary libraries \n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import boto3\n",
    "from io import StringIO \n",
    "import json \n",
    "\n",
    "#import pyspark methods for etl process \n",
    "from pyspark.sql.functions import (\n",
    "     col , concat ,instr ,length ,substring, split, trim, when, lower ,\n",
    "     avg , round ,count , format_number,date_format ,\n",
    "     year , month , weekday,udf ,max , min , sum \n",
    ")\n",
    "from pyspark.sql.types import FloatType ,DecimalType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209752fc-c476-4fe8-b040-ee0fb4e5eced",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DatabaseManager: \n",
    "## Manage Database Connections and Data Loading Operations in AWS RDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d529d49-7802-4c25-9c92-39b940ed371e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DatabaseManager:\n",
    "    \"\"\"\n",
    "    DatabaseManager class is responsible for managing the connection between Databricks and AWS RDS.\n",
    "    It provides methods to:\n",
    "    - Create and manage database connections\n",
    "    - Load data from S3 into RDS tables\n",
    "    - Create multiple tables in RDS\n",
    "    - Handle the lifecycle of the database connection\n",
    "    \"\"\"\n",
    "    global logger , func \n",
    "    # Constructor initializes the database connection and cursor\n",
    "    def __init__(self):\n",
    "        self._conn, self._conn_str = self.create_connection() # Establishes connection to the database\n",
    "        self._cursor = self._conn.cursor() # Initializes the cursor for executing SQL queries\n",
    "    @func.logger\n",
    "    def create_connection(self):\n",
    "        \"\"\"Creates a connection to the RDS database using credentials from environment variables\"\"\"\n",
    "        conn_params = {\n",
    "            \"host\": os.getenv(\"HOST\"),\n",
    "            \"port\": int(os.getenv(\"PORT\")),\n",
    "            \"database\": os.getenv(\"DBASE\"),\n",
    "            \"user\": os.getenv(\"USER\"),\n",
    "            \"password\": os.getenv(\"PASSWORD\"),\n",
    "        }\n",
    "        #Formatting a connection string for SQLAlchemy engine.\n",
    "        conn_str = \"postgresql://{0}:{1}@{2}:{3}/{4}\".format(\n",
    "            os.getenv(\"USER\"),\n",
    "            os.getenv(\"PASSWORD\"),\n",
    "            os.getenv(\"HOST\"),\n",
    "            int(os.getenv(\"PORT\")),\n",
    "            os.getenv(\"DBASE\"),\n",
    "        )\n",
    "        # Tries to establish a connection to the database \n",
    "        try:\n",
    "            conn = psycopg2.connect(**conn_params)\n",
    "            return conn, conn_str\n",
    "        except psycopg2.DatabaseError as ex:\n",
    "            logger.exception(str(ex), exc_info=True) #logs an exception if connection failed \n",
    "\n",
    "    @func.logger \n",
    "    def set_source(self, tables: tuple, s3_file_keys: tuple) -> None:\n",
    "        \"\"\" Sets the source for loading data: table names and corresponding S3 file keys \"\"\" \n",
    "        if len(tables) != len(s3_file_keys):\n",
    "            raise Exception(\n",
    "                \"tables and s3_file_keys  must have the same number of elements.\"\n",
    "            )\n",
    "\n",
    "        self.tables = tables # Stores `Table Names` \n",
    "        self.s3_file_keys = s3_file_keys # Stores `S3 file keys` \n",
    "    @func.logger \n",
    "    def get_db_cursor(self):\n",
    "        \"\"\"Returns the current cursor or create new one if not already initialized\"\"\" \n",
    "        if self._cursor:\n",
    "            return self._cursor\n",
    "        self._cursor = self._conn.cursor() # Initialize new cursor \n",
    "        return self._cursor\n",
    "    \n",
    "    @func.logger\n",
    "    def load_data_from_s3(self):\n",
    "        \"\"\"Load the data from S3 into RDS tables specified in `tables`\"\"\" \n",
    "        try:\n",
    "            if all([self.tables, self.s3_file_keys]): \n",
    "                s3_client = boto3.client(\"s3\") # Initializes S3 Client \n",
    "                engine = create_engine(self._conn_str) # Create SQLAlchemy engine for database \n",
    "                with engine.connect() as connection:\n",
    "                    transaction = connection.begin() # Starts a Transaction \n",
    "                    try:\n",
    "                        for tname, s3_file in zip(self.tables, self.s3_file_keys):\n",
    "                            #Reads the S3 Object(csv file) and loads into a pandas Dataframe \n",
    "                            s3_object = s3_client.get_object(\n",
    "                                Bucket=\"ecommerce-data-source\", Key=s3_file\n",
    "                            )\n",
    "                            data = s3_object[\"Body\"].read().decode(\"utf-8\")\n",
    "                            df = pd.read_csv(StringIO(data))\n",
    "\n",
    "                            # Loads data from the DataFrame into the corresponding RDS table\n",
    "                            df.to_sql(tname, engine, if_exists=\"append\", index=False) \n",
    "\n",
    "                            logger.info(\"Data Loaded in `{0}` Table, source:{1}\".format(tname, s3_file))\n",
    "                            print(\"Data Loaded in `{0}` Table, source:{1}\".format(tname, s3_file))\n",
    "                        transaction.commit() # Commit the Transaction if all operations are successfull.\n",
    "                    except Exception as ex:\n",
    "                        transaction.rollback() # Rollback the transaction in case of error \n",
    "                        logger.exception(str(ex), exc_info=True)\n",
    "                        print(str(ex))\n",
    "                        raise\n",
    "            else:\n",
    "                print(\"TableNames or S3 URI is Empty\")\n",
    "        except Exception as e:\n",
    "            logger.exception(str(e), exc_info=True)\n",
    "            raise\n",
    "        finally:\n",
    "            if \"engine\" in locals(): \n",
    "                engine.dispose()  # Disposes the SQLAlchemy engine after use\n",
    "    @func.logger \n",
    "    def create_multiple_tables(self, table_definitions: list) -> None:\n",
    "        \"\"\"Create multiple tables in RDS database using the provided table_definitions\"\"\"\n",
    "        try:\n",
    "            cursor = self.get_db_cursor() # Gets the DB cursor for executing SQL quries\n",
    "            for table_def in table_definitions:\n",
    "                cursor.execute(table_def) # Execute the SQL command to create each table\n",
    "                logger.debug(\n",
    "                    f\"Table `{table_def.split('(')[0].split(' ')[-1]}` Created.\"\n",
    "                )\n",
    "                print(f\"Table `{table_def.split('(')[0].split(' ')[-1]}` Created \")\n",
    "            self._conn.commit()\n",
    "\n",
    "        except Exception as e:\n",
    "            self._conn.rollback() #Rollback the transaction in case of error \n",
    "            logger.exception(str(ex), exc_info=True) #log the exception \n",
    "            raise\n",
    "    @func.logger \n",
    "    def close_connection(self):\n",
    "        \"\"\" Closes the database cursor and connection \"\"\"\n",
    "        if self._cursor:\n",
    "            self._cursor.close()\n",
    "        if self._conn:\n",
    "            self._conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd52486d-e063-48fd-84d9-746a297b149a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Manage RDS Database Table Creation and Ingest Data from S3 to PostgreSQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dc15f20-3002-4631-9570-f789932ab354",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@func.logger \n",
    "def migrate_raw_data_to_rds():\n",
    "    \"\"\"\n",
    "    `migrate_raw_data_to_rds` function to handle the database table creation and data ingestion process from S3 to PostgreSQL (RDS).\n",
    "    It performs the following steps:\n",
    "    1. Initializes the DatabaseManager object.\n",
    "    2. Creates required tables in PostgreSQL.\n",
    "    3. Configures the source tables and S3 file URIs.\n",
    "    4. Loads data from S3 to PostgreSQL.\n",
    "    5. Closes the database connection.\n",
    "    \"\"\"\n",
    "    #Initialize DatabaseManager Object \n",
    "    logger.debug(\"`migrate_raw_data_to_rds()` function execution starting..\") \n",
    "    try:\n",
    "        # Instantiate DatabaseManager to manage PostgreSQL connection and operations\n",
    "        postgres = DatabaseManager() \n",
    "\n",
    "        # Create required  tables in RDS database using Predefined SQL definitions\n",
    "        postgres.create_multiple_tables(table_definitions) \n",
    "\n",
    "        logger.debug(\"Required tables are created in PostgresSQL Database.\") \n",
    "\n",
    "        # Set the required table names  and s3 file keys for data loading \n",
    "        postgres.set_source(table_names,s3_file_keys) \n",
    "\n",
    "        logger.debug(\"TableNames and S3 Resource URI's are Configured.\")\n",
    "\n",
    "        # Load the data from s3 to postgresSQL databasae (RDS)\n",
    "        postgres.load_data_from_s3() \n",
    "        \n",
    "        logger.debug(\"Data Loaded into PostgresSQL Database.\")\n",
    "       \n",
    "        # Closes the PostgreSQL connection \n",
    "        postgres.close_connection() \n",
    "\n",
    "        logger.debug(\"Database connection has been closed\")\n",
    "        \n",
    "    except Exception as ex :\n",
    "        logger.exception(str(ex),exc_info=True) \n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        logger.debug(\"`migrate_raw_data_to_rds()` function execution ending..\")\n",
    "       \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   migrate_raw_data_to_rds() \n",
    "   pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224086ee-d6b2-4b01-b1a4-cd5c09c49c8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql  \n",
    "-- Create Databse\n",
    "CREATE DATABASE IF NOT EXISTS ecommerce_db \n",
    "LOCATION 'dbfs:/dbfs/ECOMMERCE_DB';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a43bce-9ec6-4a3a-a549-035a5e55f6da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Fetch product data and broadcast it to all workers\n",
    "product_prices_df = spark.sql('SELECT DISTINCT product_id, price FROM delta.`dbfs:/dbfs/ECOMMERCE_DB/products_v1`')\n",
    "\n",
    "# Store product-id and price as dictionary\n",
    "\n",
    "product_prices = {row['product_id']: row['price'] for row in product_prices_df.collect()}\n",
    "\n",
    "# Broadcast the dictionary\n",
    "broadcasted_prices = spark.sparkContext.broadcast(product_prices)\n",
    "\n",
    "### Custom user defined functions \n",
    "@udf(FloatType())\n",
    "def getTotalCostUdf(product_id: int, qty: int):\n",
    "    \"\"\"\n",
    "    Calculate the total cost for a product based on its price and quantity.\n",
    "    \n",
    "    Args:\n",
    "        product_id (int): The ID of the product.\n",
    "        qty (int): The quantity of the product purchased.\n",
    "\n",
    "    Returns:\n",
    "        float: The total cost (price * qty) or 0.0 if the quantity is zero or the product is not found.\n",
    "    \"\"\"\n",
    "    # Fetch the price from the broadcasted dictionary of prices; default to 0.0 if product_id is missing\n",
    "    price = float(broadcasted_prices.value.get(product_id, 0.0))\n",
    "    \n",
    "    # Return the total cost, or 0.0 if quantity is 0\n",
    "    return price * qty if qty else 0.0\n",
    "\n",
    "\n",
    "\n",
    "@udf(FloatType())\n",
    "def getTotalDiscountUdf(product_id: int, qty: int, discount) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the total discount amount for a product based on its price, quantity, and discount rate.\n",
    "    \n",
    "    Args:\n",
    "        product_id (int): The ID of the product.\n",
    "        qty (int): The quantity of the product purchased.\n",
    "        discount (float or str): The discount rate (as a percentage or float value) to be applied.\n",
    "\n",
    "    Returns:\n",
    "        float: The total discounted amount or 0.0 if no discount is provided or product is not found.\n",
    "    \"\"\"\n",
    "    # If no discount is provided, return 0.0\n",
    "    if not discount:\n",
    "        return 0.0\n",
    "    \n",
    "    # Fetch the price from the broadcasted dictionary of prices; default to 0.0 if product_id is missing\n",
    "    price = float(broadcasted_prices.value.get(product_id, 0.0))\n",
    "    \n",
    "    # Calculate the discount amount on the price\n",
    "    discount_price = price * float(discount)\n",
    "    \n",
    "    # Return the total discounted amount (discounted price * qty) or 0.0 if no valid discount\n",
    "    return discount_price * float(qty) if discount_price else 0.0\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "963308d5-6172-4d7e-bdc2-3ab3b25b8279",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ETL Extraction: Read Data from RDS and Store as Delta Tables in Databricks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b99fd34f-3b34-48c8-b580-5e27f13c6e47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Gets the data from RDS database \n",
    "@func.logger \n",
    "def etl_extraction():\n",
    "    \"\"\"Reads data from the RDS database and stores it as Delta tables in Databricks.\"\"\" \n",
    "    \n",
    "    #Reads the RDS `Tables` one by one\n",
    "    def ingest_data_from_rds(tables):\n",
    "        options = {\n",
    "            'url' :f'{os.environ[\"END_POINT\"]}', \n",
    "            'user' : f'{os.environ[\"USER\"]}',\n",
    "            'password' : f'{os.environ[\"PASSWORD\"]}',\n",
    "            'driver' :'org.postgresql.Driver'\n",
    "        }\n",
    "        try:\n",
    "            # Iterate through each table name and load data\n",
    "            for tname in tables:\n",
    "                # Read the data from the specified RDS table into a DataFrame\n",
    "                df = (spark.read\n",
    "                    .format('jdbc')\n",
    "                    .options(**options)\n",
    "                    .option('dbtable',tname).load())\n",
    "                # Write the Dataframe as Delta tables \n",
    "                df.write\\\n",
    "                    .mode('overwrite')\\\n",
    "                    .format('delta')\\\n",
    "                    .saveAsTable('ecommerce_db.'+tname) \n",
    "                logger.debug(f'Delta Table {tname} Created.')\n",
    "                print(f'Delta Table {tname} Created.')\n",
    "        except Exception as e:\n",
    "            logger.exception(str(e),exc_info=True)\n",
    "            raise # re raise the exception \n",
    "\n",
    "    ingest_data_from_rds(table_names) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f8d7831-1b07-4afb-baa3-580d18f996cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ETL Transformation: Reading Data from Delta Tables and Processing It\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da75ea1b-fd6f-4f90-9915-e8d18f391d15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## data transformation\n",
    "@func.logger \n",
    "def etl_transform_v1():\n",
    "    \"\"\"\n",
    "    Transform the DataFrames from the E-Commerce database.\n",
    "    This function performs transformations on the following tables:\n",
    "    1. categories_df\n",
    "    2. subcategories_df\n",
    "    3. customers_df\n",
    "    4. products_df\n",
    "    5. products_ratings_df\n",
    "    6. orders_df\n",
    "    7. order_items_df\n",
    "    8. returned_products_df\n",
    "    \"\"\"\n",
    "\n",
    "    ### 1.Normalize `categories` table\n",
    "\n",
    "    categories_df = spark.sql(\n",
    "        \"SELECT * FROM delta. `dbfs:/dbfs/ECOMMERCE_DB/categories`;\"\n",
    "    )  # denormalized table\n",
    "\n",
    "    # create separate table for subcategory to avoid redundancy\n",
    "    subcategories_df = categories_df.select(\n",
    "        \"sub_category_id\", \"sub_category_name\", \"category_id\"\n",
    "    )\n",
    "    # remove unnecssary columns from categories table\n",
    "    categories_df = (\n",
    "        categories_df.drop(\"sub_category_id\", \"sub_category_name\")\n",
    "        .distinct()\n",
    "        .orderBy(\"category_id\")\n",
    "    )\n",
    "\n",
    "    ### 2. transform customers table\n",
    "\n",
    "    customers_df = spark.sql(\n",
    "        \"SELECT * FROM delta. `dbfs:/dbfs/ECOMMERCE_DB/customers`;\"\n",
    "    )\n",
    "    # remove duplcate records ,concat first_name and last_name column as full_name ,extract the domain from email\n",
    "    customers_df = (\n",
    "        customers_df.distinct()\n",
    "        .withColumn(\"full_name\", concat(col(\"first_name\"), col(\"last_name\")))\n",
    "        .withColumn(\"domain\", split(\"email\", \"@\")[1])\n",
    "        .drop('first_name','last_name')\n",
    "    )\n",
    "\n",
    "    ### 3. transform customer_products_ratings table\n",
    "    products_ratings_df = spark.sql(\n",
    "        \"SELECT * FROM delta. `dbfs:/dbfs/ECOMMERCE_DB/customers_ratings`;\"\n",
    "    )\n",
    "\n",
    "    # trim whitespace and standartize review column\n",
    "    products_ratings_df = products_ratings_df.withColumn(\n",
    "        \"review\", lower(trim(\"review\"))\n",
    "    )\n",
    "\n",
    "    # calculate average rating for products\n",
    "    avg_ratings_df = products_ratings_df.groupBy(\"product_id\").agg(\n",
    "        round(avg(\"ratings\"), 2).alias(\"avg_rating\"),\n",
    "        count(\"product_id\").alias(\"reviews_count\"),\n",
    "    )\n",
    "    # join both dataframes  and # fill default values as 0  for null value in avg_rating column\n",
    "    products_ratings_df = products_ratings_df.join(\n",
    "        other=avg_ratings_df, on=\"product_id\", how=\"left\"\n",
    "    ).fillna({\"avg_rating\": 0})\n",
    "\n",
    "    ### 4. transform products table\n",
    "\n",
    "    products_df = spark.sql(\n",
    "        \"SELECT * FROM delta. `dbfs:/dbfs/ECOMMERCE_DB/products_v1`;\"\n",
    "    )\n",
    "    # calculate average price for products\n",
    "    avg_price_df = products_df.groupBy(\"subcategory_id\").agg(\n",
    "        round(avg(\"price\"), 2).alias(\"avg_price\")\n",
    "    )\n",
    "    products_df = (\n",
    "        products_df.distinct()\n",
    "        .withColumn(\"product_category\", trim(split(\"name\", \"-\")[0]))\n",
    "        .withColumn(\"description\", trim(col(\"description\")))\n",
    "        .join(avg_price_df, \"subcategory_id\", \"left\")\n",
    "        .withColumn(\n",
    "            \"price_range\",\n",
    "            when(col(\"price\") > col(\"avg_price\"), \"High\")\n",
    "            .when(col(\"price\") == col(\"avg_price\"), \"Medium\")\n",
    "            .otherwise(\"Low\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ### 5. transform orders table\n",
    "    orders_df = spark.sql(\"SELECT * FROM delta. `dbfs:/dbfs/ECOMMERCE_DB/orders`;\")\n",
    "    orders_df = (\n",
    "        orders_df\n",
    "        # Remove unnecessary columns and Remove duplicates\n",
    "        .drop(\"order_id_surrogate\", \"campaign_id\")\n",
    "        .distinct()\n",
    "        # Round 'amount' to 2 decimal places\n",
    "        .withColumn(\"amount\", round(\"amount\", 2))\n",
    "        # change order_date format to yyyy-MM-dd\n",
    "        .withColumn(\"order_date\", date_format(\"order_date\", \"yyyy-MM-dd\"))\n",
    "        # Extract year,month , weekday from 'order_date'\n",
    "        .withColumn(\"year\", year(\"order_date\"))\n",
    "        .withColumn(\"month\", month(\"order_date\"))\n",
    "        .withColumn(\"week_day\", weekday(\"order_date\"))\n",
    "    )\n",
    "\n",
    "    ### 5. transform order_item table\n",
    "    order_items_df = spark.sql(\n",
    "        \"SELECT * FROM delta. `dbfs:/dbfs/ECOMMERCE_DB/order_items`;\"\n",
    "    )\n",
    "\n",
    "    order_items_df = (\n",
    "        order_items_df.withColumn(\"subtotal\", round(\"subtotal\", 2))\n",
    "        .withColumn(\"discount\", round(\"discount\", 2))\n",
    "        .withColumn(\n",
    "            \"gross_total\", round(getTotalCostUdf(col(\"product_id\"), col(\"quantity\")), 2)\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"discount_amount\",\n",
    "            round(\n",
    "                getTotalDiscountUdf(\n",
    "                    col(\"product_id\"), col(\"quantity\"), col(\"discount\")\n",
    "                ),\n",
    "                2,\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\"net_total\", round(col(\"gross_total\") - col(\"discount_amount\"), 2))\n",
    "    )\n",
    "    returned_products_df = spark.sql(\"SELECT * FROM delta. `dbfs:/dbfs/ECOMMERCE_DB/returned_products`;\")\n",
    "    suppliers_df = spark.sql(\"SELECT * FROM delta. `dbfs:/dbfs/ECOMMERCE_DB/supplier`;\")\n",
    "    payment_methods_df = spark.sql(\"SELECT * FROM delta. `dbfs:/dbfs/ECOMMERCE_DB/payment_methods`;\")\n",
    "    return(\n",
    "        categories_df,subcategories_df,customers_df,products_df , \n",
    "        products_ratings_df,orders_df, order_items_df ,returned_products_df ,\n",
    "        suppliers_df , payment_methods_df\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9494d1-c0b9-4a21-a576-a622abbb24e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create a Fact Table Using Transformed DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e97c2900-eb2d-42bf-b8ea-19e2db4354f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@func.logger \n",
    "def etl_transform_v2():\n",
    "    \"\"\"\n",
    "    Transform the DataFrames from the E-Commerce database into a fact table.\n",
    "    This function performs transformations and aggregations on the following tables:\n",
    "    1. products_df\n",
    "    2. products_ratings_df\n",
    "    3. order_items_df\n",
    "    4. returned_products_df\n",
    "    \"\"\"\n",
    "    # Create initial fact table with product ratings\n",
    "    fact_v1 = (\n",
    "        products_df.alias(\"p_df\")\n",
    "        .join(products_ratings_df.alias(\"cpr_df\"), \"product_id\", \"left\")\n",
    "        .groupBy(\"p_df.product_id\")\n",
    "        .agg(\n",
    "            # Average rating\n",
    "            round(avg(\"cpr_df.ratings\"), 2).alias(\"average_rating\"),\n",
    "            max(\"cpr_df.ratings\").alias(\"highest_rating\"),\n",
    "            min(\"cpr_df.ratings\").alias(\"lowest_rating\"),\n",
    "            count(\"cpr_df.sentiment\").alias(\"totol_reviews\"),\n",
    "            sum(when(col(\"cpr_df.sentiment\") == \"good\", 1).otherwise(0)).alias(\n",
    "                \"positive_reviews_count\"\n",
    "            ),\n",
    "            sum(when(col(\"cpr_df.sentiment\") == \"bad\", 1).otherwise(0)).alias(\n",
    "                \"negative_reviews_count\"\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # Aggregate sales data from order items\n",
    "    temp_v1 = (\n",
    "        fact_v1.alias(\"p_df\")\n",
    "        .join(order_items_df.alias(\"oi_df\"), \"product_id\", \"left\")\n",
    "        .groupBy(\"p_df.product_id\")\n",
    "        .agg(\n",
    "            sum(\"quantity\").alias(\"total_quantities_sold\"),\n",
    "            round(sum(\"discount_amount\"), 3).alias(\"total_discount_amount\"),\n",
    "            round(sum(\"gross_total\"), 3).alias(\"gross_total_amount\"),\n",
    "            round(sum(\"net_total\"), 3).alias(\"net_total_amount\"),\n",
    "        )\n",
    "    )\n",
    "    # Aggregate returned-products data\n",
    "    temp_v2 = (\n",
    "        returned_products_df.groupBy(\"product_id\")\n",
    "        .count()\n",
    "        .withColumnRenamed(\"count\", \"no_of_returns\")\n",
    "        .orderBy(\"product_id\")\n",
    "    )\n",
    "    # Create the final fact table by joining all aggregations\n",
    "    fact_v2 = (\n",
    "        fact_v1.join(temp_v1, \"product_id\", \"left\")\n",
    "        .join(temp_v2, \"product_id\", \"left\")\n",
    "        .fillna(0) # Fill nulls with 0 for review and sales counts\n",
    "    )\n",
    "    return fact_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b2a9f7-2544-448b-8456-ea2fb86d0db2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Validating DataFrames: Handling Nulls and Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c89d21-a93d-4f6e-8ac9-d9189146d642",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@func.logger\n",
    "def etl_validation():\n",
    "    \"\"\"\n",
    "    Validate the transformed DataFrames by ensuring there are no duplicate records\n",
    "    and filling in default values for missing data.\n",
    "    \"\"\"\n",
    "    # Use global variables for transformed DataFrames\n",
    "    global categories_df, subcategories_df, customers_df, products_df, products_ratings_df, orders_df, order_items_df    \n",
    "\n",
    "    # Handle missing values \n",
    "    customers_df = customers_df.fillna(0).fillna('Unknown')\n",
    "    categories_df = categories_df.fillna(0).fillna('Not Applicable')\n",
    "    subcategories_df = subcategories_df.fillna(0).fillna('Not Applicable')\n",
    "    products_ratings_df = products_ratings_df.fillna(0).fillna('Not Provided')\n",
    "    products_df = products_df.fillna(0).fillna('Not Applicable')\n",
    "\n",
    "    orders_df = orders_df.fillna(0).fillna('Not Applicable') \n",
    "    orders_items_df =order_items_df.fillna(0).fillna('Not Applicable') \n",
    "\n",
    "    # Drop duplcaite records \n",
    "    customers_df = customers_df.distinct() \n",
    "    categories_df = categories_df.distinct() \n",
    "    subcategories_df = subcategories_df.distinct() \n",
    "    products_ratings_df = products_ratings_df.distinct() \n",
    "    products_df = products_df.distinct() \n",
    "    orders_df = orders_df.distinct() \n",
    "    orders_items_df =orders_items_df.distinct() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87db1385-4848-4021-bc5f-1b5b2c980a9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ETL Data Loading: Loading DataFrames into Snowflake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa047b7-07fb-4e36-a667-dee4869b93b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:Data-PipeLine:Table `dim_categories` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_subcategories` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_orders` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_order_items` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_products` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_customers` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_product_ratings` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_returned_products` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_payment_methods` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_suppliers` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `fact_products_summary` Created in Snowflake.\n"
     ]
    }
   ],
   "source": [
    "@func.logger \n",
    "def etl_load_to_snowflake():\n",
    "    sfTableNames = ['dim_categories','dim_subcategories','dim_orders','dim_order_items','dim_products',\n",
    "                   'dim_customers', 'dim_product_ratings','dim_returned_products','dim_payment_methods',\n",
    "                   'dim_suppliers','fact_products_summary'] \n",
    "    def df_generators():\n",
    "        yield categories_df\n",
    "        yield subcategories_df\n",
    "        yield orders_df\n",
    "        yield order_items_df\n",
    "        yield products_df \n",
    "        yield customers_df\n",
    "        yield products_ratings_df \n",
    "        yield returned_products_df \n",
    "        yield payment_methods_df \n",
    "        yield suppliers_df\n",
    "        yield fact_products_summary\n",
    "\n",
    "    # Iterate over the generator \n",
    "    try:\n",
    "        for idx ,df in enumerate(df_generators()): \n",
    "            (df \n",
    "             .write.format(\"snowflake\")\n",
    "             .mode(\"overwrite\")\n",
    "             .options(**_credentials)\n",
    "             .option('dbtable',sfTableNames[idx])\n",
    "             .save() \n",
    "            )\n",
    "            logger.debug(f'Table `{sfTableNames[idx]}` Created in Snowflake.')\n",
    "    except Exception as ex:\n",
    "        logger.exception(str(ex),exc_info=True) \n",
    "        print(ex)\n",
    "# etl_load_to_snowflake() \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f676d3d-685d-498d-b126-e664a1fb3acf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Main ETL Process: Extract, Transform, Validate, and Load Data into Snowflake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d76b15c0-560b-4736-9621-027324983db5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:Data-PipeLine:Starting ETL process...\nDEBUG:Data-PipeLine:Starting data extraction process...\nINFO:Data-PipeLine:Data extraction completed successfully.\nDEBUG:Data-PipeLine:Starting data transformation (v1)...\nINFO:Data-PipeLine:Data transformation (v1) completed successfully.\nDEBUG:Data-PipeLine:Starting fact table creation...\nINFO:Data-PipeLine:Fact table created successfully.\nDEBUG:Data-PipeLine:Starting data validation...\nINFO:Data-PipeLine:Data validation completed successfully.\nDEBUG:Data-PipeLine:Starting data load into Snowflake...\nDEBUG:Data-PipeLine:Table `dim_categories` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_subcategories` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_orders` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_order_items` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_products` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_customers` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_product_ratings` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_returned_products` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_payment_methods` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `dim_suppliers` Created in Snowflake.\nDEBUG:Data-PipeLine:Table `fact_products_summary` Created in Snowflake.\nINFO:Data-PipeLine:Data loaded into Snowflake successfully.\nDEBUG:Data-PipeLine:ETL process completed successfully.\nDEBUG:Function-Logger:{\n    \"7\": {\n        \"function_name\": \"etl_transform_v1\",\n        \"status\": \"Success\",\n        \"doc_string\": \"\\\"\\\\n    Transform the DataFrames from the E-Commerce database.\\\\n    This function performs transformations on the following tables:\\\\n    1. categories_df\\\\n    2. subcategories_df\\\\n    3. customers_df\\\\n    4. products_df\\\\n    5. products_ratings_df\\\\n    6. orders_df\\\\n    7. order_items_df\\\\n    8. returned_products_df\\\\n    \\\"\",\n        \"start_time\": \"2024-10-20 14:51:29\",\n        \"end_time\": \"2024-10-20 14:51:32\",\n        \"execution_time\": \"2.392349 seconds\",\n        \"error\": \"Not occurred\",\n        \"exception\": \"Not occurred\"\n    },\n    \"8\": {\n        \"function_name\": \"etl_transform_v2\",\n        \"status\": \"Success\",\n        \"doc_string\": \"\\\"\\\\n    Transform the DataFrames from the E-Commerce database into a fact table.\\\\n    This function performs transformations and aggregations on the following tables:\\\\n    1. products_df\\\\n    2. products_ratings_df\\\\n    3. order_items_df\\\\n    4. returned_products_df\\\\n    \\\"\",\n        \"start_time\": \"2024-10-20 14:51:32\",\n        \"end_time\": \"2024-10-20 14:51:33\",\n        \"execution_time\": \"1.123665 seconds\",\n        \"error\": \"Not occurred\",\n        \"exception\": \"Not occurred\"\n    },\n    \"9\": {\n        \"function_name\": \"etl_validation\",\n        \"status\": \"Success\",\n        \"doc_string\": \"\\\"\\\\n    Validate the transformed DataFrames by ensuring there are no duplicate records\\\\n    and filling in default values for missing data.\\\\n    \\\"\",\n        \"start_time\": \"2024-10-20 14:51:33\",\n        \"end_time\": \"2024-10-20 14:51:33\",\n        \"execution_time\": \"0.10784 seconds\",\n        \"error\": \"Not occurred\",\n        \"exception\": \"Not occurred\"\n    },\n    \"10\": {\n        \"function_name\": \"etl_load_to_snowflake\",\n        \"status\": \"Success\",\n        \"doc_string\": \"No Docstring Available\",\n        \"start_time\": \"2024-10-20 14:51:33\",\n        \"end_time\": \"2024-10-20 14:52:51\",\n        \"execution_time\": \"78.221898 seconds\",\n        \"error\": \"Not occurred\",\n        \"exception\": \"Not occurred\"\n    }\n}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try: \n",
    "        # Main entry point for the ETL process\n",
    "        logger.debug('Starting ETL process...')\n",
    "\n",
    "        # 1. Extract the data\n",
    "        logger.debug('Starting data extraction process...')\n",
    "        # etl_extraction() \n",
    "        logger.info('Data extraction completed successfully.')\n",
    "\n",
    "        # 2. Perform transformations on the DataFrames\n",
    "        logger.debug('Starting data transformation (v1)...')\n",
    "        (categories_df, subcategories_df, customers_df, products_df, \n",
    "        products_ratings_df, orders_df, order_items_df, \n",
    "        returned_products_df, suppliers_df, payment_methods_df) = etl_transform_v1()\n",
    "        \n",
    "        logger.info('Data transformation (v1) completed successfully.')\n",
    "\n",
    "        # 3. Create the fact table\n",
    "        logger.debug('Starting fact table creation...')\n",
    "        fact_products_summary = etl_transform_v2() \n",
    "        logger.info('Fact table created successfully.')\n",
    "\n",
    "        # 4. Validate the processed DataFrames\n",
    "        logger.debug('Starting data validation...')\n",
    "        etl_validation()\n",
    "        logger.info('Data validation completed successfully.')\n",
    "\n",
    "        # 5. Load the processed data into Snowflake\n",
    "        logger.debug('Starting data load into Snowflake...')\n",
    "        etl_load_to_snowflake() \n",
    "        logger.info('Data loaded into Snowflake successfully.')\n",
    "\n",
    "        logger.debug('ETL process completed successfully.')\n",
    "\n",
    "    except Exception as ex: \n",
    "        logger.exception(str(ex),exc_info=True) \n",
    "        print(ex)\n",
    "    finally:\n",
    "        # Log the function's activities (additional logging for debugging)\n",
    "        func_logger.debug(json.dumps(func.logs, indent=4))\n",
    "        func.clear_logs() \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1119747884852090,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL Process",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
